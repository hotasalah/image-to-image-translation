{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging the Pix2Pix GAN\n",
    "In Image-to-Image Translation, the task is to translate images from one domain to another by learning a mapping between the input and output images, using a training dataset of aligned or unaligned cross-domain image pairs. The focus in this project is on Paired Image-to-Image Translation.\n",
    "<br> The objective is to transform edges and hand drawings of pbjects into meaningful images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torch_snippets import *\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading and prepairing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('ShoeV2_photo'):\n",
    "    !wget https://www.dropbox.com/s/g6b6gtvmdu0h77x/ShoeV2_photo.zip\n",
    "    !unzip -q ShoeV2_photo.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_edges(img):\n",
    "    \n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    img_gray = cv2.bilateralFilter(img_gray, 5, 50, 50)\n",
    "    \n",
    "    img_gray_edges = cv2.Canny(img_gray, 45, 100)\n",
    "    img_gray_edges = cv2.bitwise_not(img_gray_edges) # invert black/white\n",
    "    img_edges = cv2.cvtColor(img_gray_edges, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    return img_edges\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "preprocess = T.Compose([\n",
    "    T.Lambda(lambda x: torch.Tensor(x.copy()).permute(2, 0, 1).to(device))\n",
    "])\n",
    "\n",
    "normalize = lambda x: (x - 127.5)/127.5\n",
    "\n",
    "class ShoesData(Dataset):\n",
    "    \n",
    "    def __init__(self, items):\n",
    "        self.items = items\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        f = self.items[ix]\n",
    "        \n",
    "        try:\n",
    "            im = read(f, 1)\n",
    "        except:\n",
    "            blank = preprocess(Blank(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "            return blank, blank\n",
    "        \n",
    "        edges = detect_edges(im)\n",
    "        im, edges = resize(im, IMAGE_SIZE), resize(edges, IMAGE_SIZE)\n",
    "        im, edges = normalize(im), normalize(edges)\n",
    "        self._draw_color_circles_on_src_img(edges, im)\n",
    "        im, edges = preprocess(im), preprocess(edges)\n",
    "        \n",
    "        return edges, im\n",
    "\n",
    "    def _draw_color_circles_on_src_img(self, img_src, img_target):\n",
    "        \n",
    "        non_white_coords = self._get_non_white_coordinates(img_target)\n",
    "        for center_y, center_x in non_white_coords:\n",
    "            self._draw_color_circle_on_src_img(img_src, img_target, center_y, center_x)\n",
    "\n",
    "    def _get_non_white_coordinates(self, img):\n",
    "        \n",
    "        non_white_mask = np.sum(img, axis=-1) < 2.75\n",
    "        non_white_y, non_white_x = np.nonzero(non_white_mask)\n",
    "        \n",
    "        # randomly sample non-white coordinates\n",
    "        n_non_white = len(non_white_y)\n",
    "        n_color_points = min(n_non_white, 300)\n",
    "        idxs = np.random.choice(n_non_white, n_color_points, replace=False)\n",
    "        non_white_coords = list(zip(non_white_y[idxs], non_white_x[idxs]))\n",
    "        \n",
    "        return non_white_coords\n",
    "\n",
    "    def _draw_color_circle_on_src_img(self, img_src, img_target, center_y, center_x):\n",
    "        \n",
    "        assert img_src.shape == img_target.shape, \"Image source and target must have same shape.\"\n",
    "        y0, y1, x0, x1 = self._get_color_point_bbox_coords(center_y, center_x)\n",
    "        color = np.mean(img_target[y0:y1, x0:x1], axis=(0, 1))\n",
    "        img_src[y0:y1, x0:x1] = color\n",
    "\n",
    "    def _get_color_point_bbox_coords(self, center_y, center_x):\n",
    "        \n",
    "        radius = 2\n",
    "        y0 = max(0, center_y-radius+1)\n",
    "        y1 = min(IMAGE_SIZE, center_y+radius)\n",
    "        x0 = max(0, center_x-radius+1)\n",
    "        x1 = min(IMAGE_SIZE, center_x+radius)\n",
    "        \n",
    "        return y0, y1, x0, x1\n",
    "\n",
    "    def choose(self):\n",
    "        return self[randint(len(self))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_items, val_items = train_test_split(Glob('ShoeV2_photo/*.png'), test_size=0.2, random_state=2)\n",
    "trn_ds, val_ds = ShoesData(train_items), ShoesData(val_items)\n",
    "\n",
    "trn_dl = DataLoader(trn_ds, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "inspect(*next(iter(trn_dl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining generator and discriminator architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing weights so that they follow a normal distribution\n",
    "def weights_init_normal(m):\n",
    "    \n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        \n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDown(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
    "        \n",
    "        super(UNetDown, self).__init__()\n",
    "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
    "        \n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_size))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        \n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetUp(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size, out_size, dropout=0.0):\n",
    "        super(UNetUp, self).__init__()\n",
    "        \n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(out_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        \n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorUNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "\n",
    "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
    "        self.down2 = UNetDown(64, 128)\n",
    "        self.down3 = UNetDown(128, 256)\n",
    "        self.down4 = UNetDown(256, 512, dropout=0.5)\n",
    "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down7 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n",
    "\n",
    "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
    "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up5 = UNetUp(1024, 256)\n",
    "        self.up6 = UNetUp(512, 128)\n",
    "        self.up7 = UNetUp(256, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(128, out_channels, 4, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "        \n",
    "        return self.final(u7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            \n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            \n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels * 2, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        \n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_train_step(real_src, real_trg, fake_trg):\n",
    "    \n",
    "    #discriminator.train()\n",
    "    d_optimizer.zero_grad()\n",
    "\n",
    "    prediction_real = discriminator(real_trg, real_src)\n",
    "    error_real = criterion_GAN(prediction_real, torch.ones(len(real_src), 1, 16, 16).cuda())\n",
    "    error_real.backward()\n",
    "\n",
    "    prediction_fake = discriminator(fake_trg.detach(), real_src)\n",
    "    error_fake = criterion_GAN(prediction_fake, torch.zeros(len(real_src), 1, 16, 16).cuda())\n",
    "    error_fake.backward()\n",
    "\n",
    "    d_optimizer.step()\n",
    "\n",
    "    return error_real + error_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train_step(real_src, fake_trg):\n",
    "    \n",
    "    #discriminator.train()\n",
    "    g_optimizer.zero_grad()\n",
    "    prediction = discriminator(fake_trg, real_src)\n",
    "\n",
    "    loss_GAN = criterion_GAN(prediction, torch.ones(len(real_src), 1, 16, 16).cuda())\n",
    "    loss_pixel = criterion_pixelwise(fake_trg, real_trg)\n",
    "    loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "    loss_G.backward()\n",
    "    g_optimizer.step()\n",
    "    \n",
    "    return loss_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "denorm = T.Normalize((-1, -1, -1), (2, 2, 2))\n",
    "\n",
    "def sample_prediction():\n",
    "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    \n",
    "    data = next(iter(val_dl))\n",
    "    real_src, real_trg = data\n",
    "    fake_trg = generator(real_src)\n",
    "    img_sample = torch.cat([denorm(real_src[0]), denorm(fake_trg[0]), denorm(real_trg[0])], -1)\n",
    "    img_sample = img_sample.detach().cpu().permute(1,2,0).numpy()\n",
    "    show(img_sample, title='Source::Generated::GroundTruth', sz=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GeneratorUNet().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()\n",
    "\n",
    "lambda_pixel = 100\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "val_dl = DataLoader(val_ds, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "log = Report(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    N = len(trn_dl)\n",
    "    \n",
    "    for bx, batch in enumerate(trn_dl):\n",
    "        real_src, real_trg = batch\n",
    "        fake_trg = generator(real_src)\n",
    "        \n",
    "        errD = discriminator_train_step(real_src, real_trg, fake_trg)\n",
    "        errG = generator_train_step(real_src, fake_trg)\n",
    "        log.record(pos=epoch+(1+bx)/N, errD=errD.item(), errG=errG.item(), end='\\r')\n",
    "\n",
    "    log.report_avgs(epoch+1)\n",
    "    [sample_prediction() for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplearning)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
